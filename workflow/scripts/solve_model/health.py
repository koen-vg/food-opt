# SPDX-FileCopyrightText: 2025 Koen van Greevenbroek
#
# SPDX-License-Identifier: GPL-3.0-or-later

"""Health objective constraints for the food systems optimization model.

This module implements health cost constraints as described in docs/health.rst.
The health objective quantifies the cost of dietary choices in terms of years
of life lost (YLL), using epidemiological dose-response relationships from
the Global Burden of Disease (GBD) Study.

Mathematical Formulation
------------------------

The health cost for cluster c and disease d is (see docs/health.rst):

    Cost_{c,d}(x) = V * (YLL_{c,d} / RR_d(x^base)) * (RR_d(x) - RR_d^ref)

where:
    - V = value per year of life lost (USD/YLL)
    - YLL_{c,d} = baseline years of life lost
    - RR_d(x) = relative risk at intake x (product over risk factors r)
    - RR_d^ref = RR at TMREL (theoretical minimum risk exposure level)
    - x^base = baseline intake

The combined relative risk is multiplicative across risk factors:

    RR_d(x) = ∏_r RR_{r,d}(x_r)

Implementation Strategy
-----------------------

To handle the nonlinear multiplicative combination, we use a two-stage
piecewise-linear approximation:

    Stage 1: Intake x_r → log(RR_{r,d}) for each (cluster, risk) pair
    Stage 2: Σ_r log(RR_{r,d}) → exp(·) → RR_d → YLL store level

Both stages use delta (incremental) variables for piecewise-linear interpolation:

    δ_j ∈ [0,1], δ_j ≤ δ_{j-1} (fill-up ordering)
    x = x_0 + Σ_j δ_j Δx_j
    f(x) = f_0 + Σ_j δ_j Δf_j

**Stage 1** requires segment indicator variables to guarantee correct interpolation
(dose-response curves may be non-convex):
    - HiGHS: binary segment indicators y_j ∈ {0,1}
    - Gurobi: continuous y_j with SOS1 constraint

**Stage 2** needs no segment indicators because exp() is convex and we minimize
RR. Convexity guarantees the optimizer naturally selects the correct delta pattern.

Code Organization
-----------------
- Data loading: _load_health_data
- Stage 1 (Intake → log(RR)):
    - _build_store_to_cluster_map: Map stores to clusters with per-capita coefficients
    - _build_intake_breakpoints: Build breakpoint grids per risk factor
    - _group_cluster_risk_pairs: Group pairs by shared breakpoints for efficiency
    - _add_stage1_constraints: Main Stage 1 logic
    - _add_stage1_delta: δ variables + segment indicators
- Stage 2 (log(RR) → YLL):
    - _build_cause_breakpoints: Build log-RR breakpoints per cause
    - _group_cluster_cause_pairs: Group pairs by shared log-RR grids
    - _add_stage2_constraints: Main Stage 2 logic
    - _add_stage2_delta: δ variables + fill-up (no indicators needed)
- Main entry point: add_health_objective
"""

from collections import defaultdict
import itertools
import logging
import math

import linopy
import numpy as np
import pandas as pd
import pypsa
import xarray as xr

from .. import constants
from ..population import get_health_cluster_population

logger = logging.getLogger(__name__)


# =============================================================================
# Module State for Auxiliary Variable Tracking
# =============================================================================

# Auxiliary variables (SOS2 segment binaries) must be removed before
# PyPSA solution assignment to avoid polluting the solved network.
HEALTH_AUX_MAP: dict[int, set[str]] = {}

# Counters for unique variable naming
_LAMBDA_GROUP_COUNTER = itertools.count()
_TOTAL_GROUP_COUNTER = itertools.count()


def _register_auxiliary_variable(m: linopy.Model, name: str) -> None:
    """Track an auxiliary variable for post-solve cleanup."""
    aux = HEALTH_AUX_MAP.setdefault(id(m), set())
    aux.add(name)


# =============================================================================
# Data Loading
# =============================================================================


def _load_health_data(
    n: pypsa.Network,
    risk_breakpoints_path: str,
    cluster_cause_path: str,
    cause_log_path: str,
    cluster_summary_path: str,
    clusters_path: str,
) -> dict:
    """Load and preprocess all health-related input data.

    Returns a dictionary with all preprocessed data needed for constraint
    construction.
    """
    risk_breakpoints = pd.read_csv(risk_breakpoints_path)
    cluster_cause = pd.read_csv(cluster_cause_path)
    cause_log_breakpoints = pd.read_csv(cause_log_path)
    cluster_summary = pd.read_csv(cluster_summary_path)
    cluster_summary["health_cluster"] = cluster_summary["health_cluster"].astype(int)
    cluster_map = pd.read_csv(clusters_path)

    # Cluster lookups
    cluster_lookup = cluster_map.set_index("country_iso3")["health_cluster"].to_dict()

    # Cluster-cause metadata (baseline YLL, RR values)
    cluster_cause_metadata = cluster_cause.set_index(["health_cluster", "cause"])

    # Get cluster population from network metadata (computed at build time)
    cluster_population = get_health_cluster_population(n)

    # Sort breakpoint tables
    risk_breakpoints = risk_breakpoints.sort_values(
        ["risk_factor", "intake_g_per_day", "cause"]
    )
    cause_log_breakpoints = cause_log_breakpoints.sort_values(["cause", "log_rr_total"])

    return {
        "risk_breakpoints": risk_breakpoints,
        "cluster_cause": cluster_cause,
        "cause_log_breakpoints": cause_log_breakpoints,
        "cluster_summary": cluster_summary,
        "cluster_cause_metadata": cluster_cause_metadata,
        "cluster_lookup": cluster_lookup,
        "cluster_population": cluster_population,
    }


# =============================================================================
# Stage 1: Intake → log(RR)
# =============================================================================


def _build_store_to_cluster_map(
    stores_df: pd.DataFrame,
    risk_factors: list[str],
    cluster_lookup: dict[str, int],
    cluster_population: dict[int, float],
) -> pd.DataFrame:
    """Map food group stores to health clusters with per-capita coefficients.

    For each food group store, computes the coefficient for converting store
    level (Mt/year) to per-capita intake (g/day):

        coeff = 10^12 / (365 * P_c)

    where P_c is the population of cluster c that country belongs to.

    Parameters
    ----------
    stores_df
        DataFrame of stores with 'food_group' and 'country' columns.
    risk_factors
        List of GBD risk factors (e.g., ['fruits', 'vegetables', ...]).
    cluster_lookup
        Mapping from country ISO3 to health cluster.
    cluster_population
        Population by health cluster.

    Returns
    -------
    pd.DataFrame
        Columns: store_name, risk_factor, country, cluster, per_capita_coeff.
    """
    # Filter for food group stores matching risk factors
    fg_stores = stores_df[stores_df["food_group"].isin(risk_factors)].copy()

    if fg_stores.empty:
        return pd.DataFrame()

    # Build mapping DataFrame using food_group column directly
    df = pd.DataFrame(
        {
            "store_name": fg_stores.index,
            "risk_factor": fg_stores["food_group"].values,
            "country": fg_stores["country"].values,
        }
    )

    # Map to cluster - fail if any countries are unmapped
    df["cluster"] = df["country"].map(cluster_lookup)
    unmapped = df[df["cluster"].isna()]["country"].unique()
    if len(unmapped) > 0:
        raise ValueError(f"Countries not mapped to health clusters: {sorted(unmapped)}")
    df["cluster"] = df["cluster"].astype(int)

    # Get cluster population - fail if any clusters have zero/missing population
    df["population"] = df["cluster"].map(cluster_population)
    zero_pop_clusters = df[df["population"].isna() | (df["population"] <= 0)][
        "cluster"
    ].unique()
    if len(zero_pop_clusters) > 0:
        raise ValueError(
            f"Health clusters with zero or missing population: {sorted(zero_pop_clusters)}"
        )

    # Per-capita coefficient: grams/megatonne / (365 * cluster_population)
    df["per_capita_coeff"] = constants.GRAMS_PER_MEGATONNE / (365.0 * df["population"])

    return df


def _build_intake_breakpoints(risk_breakpoints: pd.DataFrame) -> dict:
    """Build intake grids from RR breakpoint data.

    For each risk factor, creates:
        - intake_steps: Index of breakpoint positions
        - intake_values: xr.DataArray of intake values (g/day)
        - log_rr: DataFrame with log(RR) by (intake_step, cause)

    Parameters
    ----------
    risk_breakpoints
        DataFrame with columns: risk_factor, intake_g_per_day, cause, log_rr.

    Returns
    -------
    dict
        {risk_factor: {intake_steps, intake_values, log_rr}}
    """
    risk_data = {}
    for risk, grp in risk_breakpoints.groupby("risk_factor"):
        intakes = pd.Index(sorted(grp["intake_g_per_day"].unique()), name="intake")
        if intakes.empty:
            continue

        # Pivot to get log_rr by (intake, cause)
        pivot = (
            grp.pivot_table(
                index="intake_g_per_day",
                columns="cause",
                values="log_rr",
                aggfunc="first",
            )
            .reindex(intakes, axis=0)
            .sort_index()
        )

        intake_steps = pd.Index(range(len(intakes)), name="intake_step")
        pivot.index = intake_steps

        risk_data[risk] = {
            "intake_steps": intake_steps,
            "intake_values": xr.DataArray(
                intakes.values, coords={"intake_step": intake_steps}, dims="intake_step"
            ),
            "log_rr": pivot,
        }

    return risk_data


def _group_cluster_risk_pairs(
    store_map: pd.DataFrame, intake_data: dict
) -> dict[tuple[float, ...], list[tuple[int, str]]]:
    """Group (cluster, risk) pairs by shared intake coordinate patterns.

    Pairs with identical breakpoint grids share a single SOS2 variable set,
    improving solver efficiency.
    """
    unique_pairs = store_map[["cluster", "risk_factor"]].drop_duplicates()

    intake_groups: dict[tuple[float, ...], list[tuple[int, str]]] = defaultdict(list)
    for _, row in unique_pairs.iterrows():
        cluster = int(row["cluster"])
        risk = row["risk_factor"]

        risk_table = intake_data.get(risk)
        if risk_table is None:
            continue

        coords_key = tuple(risk_table["intake_values"].values)
        intake_groups[coords_key].append((cluster, risk))

    return intake_groups


def _add_stage1_constraints(
    m: linopy.Model,
    store_map: pd.DataFrame,
    intake_groups: dict[tuple[float, ...], list[tuple[int, str]]],
    intake_data: dict,
    store_level_var: xr.DataArray,
    solver_name: str,
    value_per_yll: float,
) -> dict[tuple[int, str], linopy.LinearExpression]:
    """Add Stage 1 constraints: store level → log(RR_{r,d}).

    Stage 1 transforms food group store levels into log relative risk values
    using piecewise-linear interpolation with delta (incremental) variables.

    Both solvers use the delta formulation:
        - δ_j ∈ [0,1], δ_j ≤ δ_{j-1} (fill-up ordering)
        - x = x_0 + Σ_j δ_j Δx_j, log(RR) = f_0 + Σ_j δ_j Δf_j

    To guarantee correct interpolation (only one fractional δ), segment
    indicator variables y_j are added with solver-dependent constraints:
        - HiGHS: binary y_j ∈ {0,1}
        - Gurobi: continuous y_j with SOS1 constraint

    Parameters
    ----------
    m
        The linopy model.
    store_map
        Store mapping from _build_store_to_cluster_map.
    intake_groups
        (cluster, risk) pairs grouped by intake coordinates.
    intake_data
        Breakpoint data from _build_intake_breakpoints.
    store_level_var
        Store level variables (food group stores).
    solver_name
        Solver name for formulation selection.
    value_per_yll
        Value per YLL; if zero, skip degeneracy perturbation.

    Returns
    -------
    dict
        {(cluster, cause): Σ_r log(RR_{r,d})} expressions for Stage 2.
    """
    log_rr_totals: dict[tuple[int, str], linopy.LinearExpression] = {}

    # Process (cluster, risk) pairs in groups that share the same intake breakpoints.
    # This batches constraint creation for efficiency - pairs with identical
    # breakpoint grids can share a single variable array.
    for _intake_grid, cluster_risk_pairs in intake_groups.items():
        # Get risk data for this group (all pairs share same breakpoints)
        risk = cluster_risk_pairs[0][1]
        risk_table = intake_data[risk]
        intake_values = risk_table["intake_values"]

        # Build labels and dataframes for this group
        cluster_risk_labels = [
            f"c{cluster}_r{risk}" for cluster, risk in cluster_risk_pairs
        ]
        cluster_risk_index = pd.Index(cluster_risk_labels, name="cluster_risk")
        pairs_df = pd.DataFrame(cluster_risk_pairs, columns=["cluster", "risk_factor"])
        pairs_df["cluster_risk"] = cluster_risk_labels

        # -----------------------------------------------------------------------
        # Build intake expression from stores
        # -----------------------------------------------------------------------
        # Each country c in cluster C has a food group store with level s_c (Mt/year).
        # Cluster intake I_C is the population-weighted average:
        #
        #     I_C = Σ_{c∈C} s_c * (10^12 g/Mt) / (365 days * P_C persons)
        #
        # where P_C is cluster population.

        stores_with_labels = store_map.merge(
            pairs_df, on=["cluster", "risk_factor"], how="inner"
        )

        if stores_with_labels.empty:
            continue

        store_names = stores_with_labels["store_name"].values
        per_capita_coeffs = xr.DataArray(
            stores_with_labels["per_capita_coeff"].values,
            coords={"name": store_names},
            dims="name",
        )
        grouper = xr.DataArray(
            stores_with_labels["cluster_risk"].values,
            coords={"name": store_names},
            dims="name",
            name="cluster_risk",
        )

        # Aggregated store level expression by cluster_risk (g/person/day)
        store_expr = (
            (store_level_var.sel(name=store_names) * per_capita_coeffs)
            .groupby(grouper)
            .sum()
        )

        # -----------------------------------------------------------------------
        # Build log(RR) breakpoint data
        # -----------------------------------------------------------------------
        log_rr_frames = [
            intake_data[risk]["log_rr"] for _cluster, risk in cluster_risk_pairs
        ]

        if not log_rr_frames:
            continue

        # Concat along cluster_risk dimension
        combined_log_rr = pd.concat(
            log_rr_frames,
            keys=cluster_risk_index,
            names=["cluster_risk", "intake_step"],
        )

        # Check for missing log_rr values
        if combined_log_rr.isna().any().any():
            missing = combined_log_rr[combined_log_rr.isna().any(axis=1)]
            raise ValueError(
                f"Missing log_rr values in risk breakpoints for {len(missing)} "
                f"(cluster_risk, intake_step) combinations; first few: "
                f"{list(missing.index[:5])}"
            )

        # Convert to DataArray: (cluster_risk, intake_step, cause)
        stacked_log_rr = combined_log_rr.stack()
        stacked_log_rr.index.names = ["cluster_risk", "intake_step", "cause"]
        log_rr_by_intake = xr.DataArray.from_series(stacked_log_rr)

        # -----------------------------------------------------------------------
        # Delta formulation (same structure for both solvers)
        # -----------------------------------------------------------------------
        log_rr_contrib = _add_stage1_delta(
            m=m,
            store_expr=store_expr,
            intake_values=intake_values,
            log_rr_by_intake=log_rr_by_intake,
            cluster_risk_index=cluster_risk_index,
            risk_label=str(risk),
            value_per_yll=value_per_yll,
            solver_name=solver_name,
        )

        # -----------------------------------------------------------------------
        # Accumulate log(RR) by cluster
        # -----------------------------------------------------------------------
        # The multiplicative RR relationship becomes additive in log space:
        #     RR_d = ∏_r RR_{r,d}  ⟹  log(RR_d) = Σ_r log(RR_{r,d})

        cluster_by_label = pairs_df.set_index("cluster_risk")["cluster"]
        present_labels = log_rr_contrib.coords["cluster_risk"].values
        cluster_grouper = xr.DataArray(
            cluster_by_label.loc[present_labels].values,
            coords={"cluster_risk": present_labels},
            dims="cluster_risk",
            name="cluster",
        )

        # Σ_r log(RR_{r,d}) for each (cluster, cause)
        log_rr_by_cluster = log_rr_contrib.groupby(cluster_grouper).sum()

        # Store expressions for Stage 2
        causes = log_rr_by_cluster.coords["cause"].values
        clusters = log_rr_by_cluster.coords["cluster"].values

        for c in clusters:
            for cause in causes:
                expr = log_rr_by_cluster.sel(cluster=c, cause=cause)
                key = (c, cause)

                if key in log_rr_totals:
                    log_rr_totals[key] = log_rr_totals[key] + expr
                else:
                    log_rr_totals[key] = expr

    return log_rr_totals


def _add_stage1_delta(
    m: linopy.Model,
    store_expr: linopy.LinearExpression,
    intake_values: xr.DataArray,
    log_rr_by_intake: xr.DataArray,
    cluster_risk_index: pd.Index,
    risk_label: str,
    value_per_yll: float,
    solver_name: str,
) -> linopy.LinearExpression:
    """Stage 1 delta formulation with segment indicators.

    Creates δ variables with fill-up constraints for piecewise-linear interpolation:
        x = x_0 + Σ_j δ_j Δx_j,  f(x) = f_0 + Σ_j δ_j Δf_j

    Segment indicator variables y_j guarantee correct interpolation:
        - HiGHS: binary y_j ∈ {0,1} with Σy = 1
        - Gurobi: continuous y_j with SOS1 constraint

    Linking constraints tie δ and y:
        - δ_i ≥ Σ_{k>i} y_k  (δ_i = 1 if active segment is later)
        - δ_i ≤ Σ_{k≥i} y_k  (δ_i = 0 if active segment is earlier)

    Returns log(RR) expression indexed by (cluster_risk, cause).
    """
    intake_steps = intake_values.coords["intake_step"]
    n_points = len(intake_steps)
    n_segments = n_points - 1
    segment_dim = "intake_step_seg"
    segment_coords = pd.Index(range(n_segments), name=segment_dim)

    # Compute segment widths: Δx_j = x_{j+1} - x_j
    delta_x = intake_values.diff("intake_step")
    delta_x = delta_x.rename({"intake_step": segment_dim})
    delta_x = delta_x.assign_coords({segment_dim: segment_coords})

    group_id = next(_LAMBDA_GROUP_COUNTER)

    # Create δ variables
    delta_var = m.add_variables(
        lower=0,
        upper=1,
        coords=[cluster_risk_index, segment_coords],
        name=f"health_delta_group_{group_id}_{risk_label}",
    )
    _register_auxiliary_variable(m, delta_var.name)

    # Fill-up constraints: δ_j ≤ δ_{j-1} for j ≥ 1
    # Vectorized: use roll() to shift values, then compare slices with aligned coords
    if n_segments > 1:
        # Roll shifts values circularly by -1: [δ0, δ1, ..., δn-1] -> [δ1, δ2, ..., δn-1, δ0]
        # Select first n-1 elements to get [δ1, δ2, ..., δn-1] with coords [0, 1, ..., n-2]
        delta_rolled = delta_var.roll({segment_dim: -1})
        delta_current = delta_rolled.isel(
            {segment_dim: slice(0, -1)}
        )  # δ[j] for j=1..n-1
        delta_prev = delta_var.isel({segment_dim: slice(0, -1)})  # δ[j-1] for j=1..n-1

        # Both have same coords [0, 1, ..., n-2], so comparison works directly
        # Constraint: δ[j] ≤ δ[j-1]
        m.add_constraints(
            delta_current <= delta_prev,
            name=f"health_delta_fillup_{group_id}_{risk_label}",
        )

    # -----------------------------------------------------------------------
    # Segment indicator variables for correct interpolation
    # -----------------------------------------------------------------------
    # y_j indicates segment j is "active" (contains the fractional δ)
    # Exactly one segment is active: Σ y_j = 1
    #
    # For HiGHS: binary variables
    # For Gurobi: continuous variables with SOS1 constraint
    use_binary = solver_name.lower() == "highs"

    if use_binary:
        y_var = m.add_variables(
            binary=True,
            coords=[cluster_risk_index, segment_coords],
            name=f"health_segment_ind_{group_id}_{risk_label}",
        )
    else:
        y_var = m.add_variables(
            lower=0,
            upper=1,
            coords=[cluster_risk_index, segment_coords],
            name=f"health_segment_ind_{group_id}_{risk_label}",
        )
        # Add SOS1 constraint: at most one y_j non-zero per cluster_risk
        m.add_sos_constraints(y_var, sos_type=1, sos_dim=segment_dim)

    _register_auxiliary_variable(m, y_var.name)

    # Exactly one segment active
    m.add_constraints(
        y_var.sum(segment_dim) == 1,
        name=f"health_segment_sum_{group_id}_{risk_label}",
    )

    # Linking constraints between δ and y
    # For segment j active (y_j = 1):
    #   - δ_0 = δ_1 = ... = δ_{j-1} = 1 (all before are full)
    #   - δ_j ∈ [0, 1] (the active one is fractional)
    #   - δ_{j+1} = ... = δ_{n-1} = 0 (all after are empty)
    #
    # Constraints:
    #   δ_i ≥ Σ_{k=i+1}^{n-1} y_k  (δ_i = 1 if active segment is later than i)
    #   δ_i ≤ Σ_{k=i}^{n-1} y_k    (δ_i = 0 if active segment is before i)
    #
    # Vectorized implementation using suffix sums computed via matrix multiplication.

    # Build suffix sum coefficient matrix: A[i,j] = 1 if j >= i
    # y_suffix[i] = Σ_{j>=i} y[j] = (A @ y)[i]
    suffix_matrix = np.triu(np.ones((n_segments, n_segments)))
    suffix_coeffs = xr.DataArray(
        suffix_matrix,
        dims=[segment_dim, "sum_over"],
        coords={segment_dim: segment_coords, "sum_over": segment_coords},
    )

    # Convert y_var to LinearExpression and rename dimension for matrix multiply.
    # We use to_linexpr() to avoid sos_dim validation issues with Variable.rename().
    y_linexpr = y_var.to_linexpr()
    y_linexpr_renamed = y_linexpr.rename({segment_dim: "sum_over"})

    # Compute suffix sums: y_suffix[i] = Σ_{j>=i} y[j]
    # Shape: (n_cluster_risk, n_segments)
    y_suffix = (y_linexpr_renamed * suffix_coeffs).sum("sum_over")

    # Upper bound constraints: δ[i] <= y_suffix[i] for all i=0..n-1
    # Both delta_var and y_suffix have same coords, so direct comparison works
    m.add_constraints(
        delta_var <= y_suffix,
        name=f"health_delta_upper_{group_id}_{risk_label}",
    )

    # Lower bound constraints: δ[i] >= y_suffix[i+1] for i=0..n-2
    # y_suffix[i+1] = Σ_{k>i} y_k (the "later" sum)
    if n_segments > 1:
        # Use roll to shift y_suffix by -1, then take first n-1 elements
        # This aligns y_suffix[i+1] with coords [0, 1, ..., n-2]
        y_later_rolled = y_suffix.roll({segment_dim: -1})
        y_later = y_later_rolled.isel({segment_dim: slice(0, -1)})  # y_suffix[i+1]
        delta_for_lower = delta_var.isel({segment_dim: slice(0, -1)})  # δ[i]

        # Both have coords [0, 1, ..., n-2], comparison works directly
        m.add_constraints(
            delta_for_lower >= y_later,
            name=f"health_delta_lower_{group_id}_{risk_label}",
        )

    # Intake balance: I_{c,r} = x_0 + Σ_j δ_j Δx_j
    x_0 = float(intake_values.isel(intake_step=0).values)
    intake_expr = x_0 + (delta_var * delta_x).sum(segment_dim)
    intake_expr = intake_expr.reindex(
        cluster_risk=store_expr.data.coords["cluster_risk"]
    )
    m.add_constraints(
        store_expr == intake_expr,
        name=f"health_delta_intake_balance_{group_id}_{risk_label}",
    )

    # Compute log(RR): log(RR_{c,r,d}) = f_0 + Σ_j δ_j Δf_j
    # Need to compute delta_f for each cause
    #
    # Manually compute differences to ensure coordinate alignment.
    # diff() can produce misaligned indices that cause broadcasting issues.
    causes = log_rr_by_intake.coords["cause"].values
    cluster_risk_vals = cluster_risk_index.values

    # Build delta_log_rr with explicit coordinates
    delta_log_rr_data = np.zeros(
        (len(cluster_risk_vals), len(segment_coords), len(causes))
    )
    for j in range(len(segment_coords)):
        delta_log_rr_data[:, j, :] = (
            log_rr_by_intake.sel(cluster_risk=cluster_risk_vals)
            .isel(intake_step=j + 1)
            .values
            - log_rr_by_intake.sel(cluster_risk=cluster_risk_vals)
            .isel(intake_step=j)
            .values
        )

    delta_log_rr = xr.DataArray(
        delta_log_rr_data,
        coords={
            "cluster_risk": cluster_risk_vals,
            segment_dim: segment_coords.values,
            "cause": causes,
        },
        dims=["cluster_risk", segment_dim, "cause"],
    )

    # f_0 is the constant offset (value at first breakpoint)
    f_0_data = (
        log_rr_by_intake.sel(cluster_risk=cluster_risk_vals).isel(intake_step=0).values
    )
    f_0 = xr.DataArray(
        f_0_data,
        coords={"cluster_risk": cluster_risk_vals, "cause": causes},
        dims=["cluster_risk", "cause"],
    )

    # Compute expression: f_0 + Σ_j δ_j Δf_j
    # Note: Use delta_contrib + f_0 (not f_0 + delta_contrib) so that linopy's
    # __add__ handles the addition properly. DataArray.__add__ doesn't know
    # how to handle LinearExpressions.
    delta_contrib = (delta_var * delta_log_rr).sum(segment_dim)
    log_rr_contrib = delta_contrib + f_0

    return log_rr_contrib


# =============================================================================
# Stage 2: log(RR) → YLL Store Level
# =============================================================================


def _build_cause_breakpoints(cause_log_breakpoints: pd.DataFrame) -> dict:
    """Build log-RR breakpoint grids by cause.

    Returns
    -------
    dict
        {cause: DataFrame with columns log_rr_total, rr_total}
    """
    return {
        cause: df.sort_values("log_rr_total")
        for cause, df in cause_log_breakpoints.groupby("cause")
    }


def _group_cluster_cause_pairs(
    cluster_cause_metadata: pd.DataFrame,
    cause_breakpoints: dict,
    cluster_population: dict[int, float],
) -> tuple[dict, dict]:
    """Group (cluster, cause) pairs by shared log-RR coordinate patterns.

    Computes absolute YLL from stored rates using planning-year population.

    Returns
    -------
    tuple
        (log_total_groups, cluster_cause_data) where:
        - log_total_groups: {coords_key: [(cluster, cause), ...]}
        - cluster_cause_data: {(cluster, cause): {yll_total, rr_ref, rr_baseline, cause_bp}}
    """
    log_total_groups: dict[tuple[float, ...], list[tuple[int, str]]] = defaultdict(list)
    cluster_cause_data: dict[tuple[int, str], dict] = {}

    for (cluster, cause), row in cluster_cause_metadata.iterrows():
        cluster = int(cluster)
        cause = str(cause)

        # Reconstruct absolute YLL from rate using planning-year population
        yll_rate_per_100k = float(row["yll_rate_per_100k"])
        pop = cluster_population[cluster]
        yll_total = (yll_rate_per_100k / constants.PER_100K) * pop

        cause_bp = cause_breakpoints.get(cause)
        if cause_bp is None:
            continue

        coords_key = tuple(cause_bp["log_rr_total"].values)
        if len(coords_key) == 1:
            raise ValueError(
                "Need at least two breakpoints for piecewise linear approximation"
            )

        log_total_groups[coords_key].append((cluster, cause))

        # Store metadata for constraint construction
        log_rr_total_ref = float(row["log_rr_total_ref"])
        log_rr_total_baseline = float(row["log_rr_total_baseline"])
        cluster_cause_data[(cluster, cause)] = {
            "yll_total": yll_total,
            "log_rr_total_ref": log_rr_total_ref,
            "rr_ref": math.exp(log_rr_total_ref),
            "rr_baseline": math.exp(log_rr_total_baseline),
            "cause_bp": cause_bp,
        }

    return log_total_groups, cluster_cause_data


def _add_stage2_constraints(
    m: linopy.Model,
    log_rr_totals: dict[tuple[int, str], linopy.LinearExpression],
    log_total_groups: dict[tuple[float, ...], list[tuple[int, str]]],
    cluster_cause_data: dict[tuple[int, str], dict],
    health_stores: pd.DataFrame,
    store_level_var: xr.DataArray,
    value_per_yll: float,
) -> int:
    """Add Stage 2 constraints: Σ_r log(RR_{r,d}) → YLL store level.

    Stage 2 transforms the summed log-RR values into YLL store levels using
    piecewise-linear interpolation to compute exp(·).

    Both solvers use the delta formulation without segment indicators:
        - δ_j ∈ [0,1], δ_j ≤ δ_{j-1} (fill-up ordering)
        - log(RR) = z_0 + Σ_j δ_j Δz_j, RR = f_0 + Σ_j δ_j Δf_j

    No segment indicators are needed because exp() is convex and we minimize
    RR (to minimize YLL cost). Convexity guarantees the optimizer naturally
    selects the correct "fill from left" delta pattern.

    The store level represents the health cost normalized by V (value per YLL):

        e_{c,d} = (RR_d - RR_d^ref) * (YLL_{c,d} / RR_d^base) * 10^{-6}

    Parameters
    ----------
    m
        The linopy model.
    log_rr_totals
        {(cluster, cause): Σ_r log(RR_{r,d})} from Stage 1.
    log_total_groups
        (cluster, cause) pairs grouped by log-RR coordinates.
    cluster_cause_data
        Metadata for each (cluster, cause) pair.
    health_stores
        DataFrame of YLL stores indexed by (health_cluster, cause).
    store_level_var
        Store energy level variables.
    value_per_yll
        Value per YLL; determines constraint type (equality vs inequality).

    Returns
    -------
    int
        Number of store level constraints added.
    """
    constraints_added = 0

    for log_rr_grid, cluster_cause_pairs in log_total_groups.items():
        log_total_vals = np.asarray(log_rr_grid, dtype=float)
        log_total_steps = pd.Index(range(len(log_total_vals)), name="log_total_step")

        # Create cluster-cause labels for vectorized operations
        cluster_cause_labels = [
            f"c{cluster}_cause{cause}" for cluster, cause in cluster_cause_pairs
        ]
        cluster_cause_index = pd.Index(cluster_cause_labels, name="cluster_cause")
        cause_label = str(cluster_cause_pairs[0][1])

        # Build breakpoint arrays for log-RR (z) and RR (exp(z))
        coeff_log_total = xr.DataArray(
            log_total_vals,
            coords={"log_total_step": log_total_steps},
            dims=["log_total_step"],
        )

        # Get RR values at breakpoints (same for all pairs in this group)
        # Use the first pair's cause_bp since all share the same breakpoints
        sample_data = cluster_cause_data[cluster_cause_pairs[0]]
        rr_vals = sample_data["cause_bp"]["rr_total"].values
        coeff_rr = xr.DataArray(
            rr_vals,
            coords={"log_total_step": log_total_steps},
            dims=["log_total_step"],
        )

        # Delta formulation for both solvers (no segment indicators needed)
        constraints_added += _add_stage2_delta(
            m=m,
            log_rr_totals=log_rr_totals,
            cluster_cause_pairs=cluster_cause_pairs,
            cluster_cause_labels=cluster_cause_labels,
            cluster_cause_index=cluster_cause_index,
            cluster_cause_data=cluster_cause_data,
            health_stores=health_stores,
            store_level_var=store_level_var,
            coeff_log_total=coeff_log_total,
            coeff_rr=coeff_rr,
            cause_label=cause_label,
            value_per_yll=value_per_yll,
        )

    return constraints_added


def _add_stage2_delta(
    m: linopy.Model,
    log_rr_totals: dict[tuple[int, str], linopy.LinearExpression],
    cluster_cause_pairs: list[tuple[int, str]],
    cluster_cause_labels: list[str],
    cluster_cause_index: pd.Index,
    cluster_cause_data: dict[tuple[int, str], dict],
    health_stores: pd.DataFrame,
    store_level_var: xr.DataArray,
    coeff_log_total: xr.DataArray,
    coeff_rr: xr.DataArray,
    cause_label: str,
    value_per_yll: float,
) -> int:
    """Stage 2 delta formulation for HiGHS (no binary variables).

    Creates δ variables with fill-up constraints for piecewise-linear interpolation
    of the exponential function (log(RR) → RR).

    Returns number of store level constraints added.
    """
    log_total_steps = coeff_log_total.coords["log_total_step"]
    n_points = len(log_total_steps)
    segment_dim = "log_total_step_seg"
    segment_coords = pd.Index(range(n_points - 1), name=segment_dim)

    # Compute segment widths: Δz_j = z_{j+1} - z_j, Δf_j = f_{j+1} - f_j
    delta_z = coeff_log_total.diff("log_total_step")
    delta_z = delta_z.rename({"log_total_step": segment_dim})
    delta_z = delta_z.assign_coords({segment_dim: segment_coords})

    delta_rr = coeff_rr.diff("log_total_step")
    delta_rr = delta_rr.rename({"log_total_step": segment_dim})
    delta_rr = delta_rr.assign_coords({segment_dim: segment_coords})

    # Create δ variables
    delta_var = m.add_variables(
        lower=0,
        upper=1,
        coords=[cluster_cause_index, segment_coords],
        name=f"health_delta_total_group_{next(_TOTAL_GROUP_COUNTER)}_{cause_label}",
    )
    _register_auxiliary_variable(m, delta_var.name)

    # Fill-up constraints: δ_j ≤ δ_{j-1} for j ≥ 1
    # Vectorized: use roll() to shift values, then compare slices with aligned coords
    if n_points > 2:
        # Roll shifts values circularly by -1: [δ0, δ1, ..., δn-1] -> [δ1, δ2, ..., δn-1, δ0]
        # Select first n-2 elements to get [δ1, δ2, ..., δn-2] with coords [0, 1, ..., n-3]
        delta_rolled = delta_var.roll({segment_dim: -1})
        delta_current = delta_rolled.isel(
            {segment_dim: slice(0, -1)}
        )  # δ[j] for j=1..n-2
        delta_prev = delta_var.isel({segment_dim: slice(0, -1)})  # δ[j-1] for j=1..n-2

        # Both have same coords, so comparison works directly
        m.add_constraints(
            delta_current <= delta_prev,
            name=f"health_delta_total_fillup_{cause_label}",
        )

    # Perturbation for delta formulation is not needed - the equality constraint
    # on log(RR) uniquely determines delta values, eliminating degeneracy.

    # Base values (at first breakpoint)
    z_0 = float(coeff_log_total.isel(log_total_step=0).values)
    f_0 = float(coeff_rr.isel(log_total_step=0).values)

    # Vectorized log-RR and RR interpolation for all (cluster, cause) pairs at once
    # log_interp_all has shape (n_cluster_cause,)
    log_interp_all = z_0 + (delta_var * delta_z).sum(segment_dim)
    rr_interp_all = f_0 + (delta_var * delta_rr).sum(segment_dim)

    # Build arrays of per-pair coefficients for vectorized store level computation
    # Indexed by cluster_cause_index
    rr_ref_vals = np.array(
        [cluster_cause_data[(c, d)]["rr_ref"] for c, d in cluster_cause_pairs]
    )
    yll_total_vals = np.array(
        [cluster_cause_data[(c, d)]["yll_total"] for c, d in cluster_cause_pairs]
    )
    rr_baseline_vals = np.array(
        [cluster_cause_data[(c, d)]["rr_baseline"] for c, d in cluster_cause_pairs]
    )

    rr_ref = xr.DataArray(
        rr_ref_vals,
        coords={"cluster_cause": cluster_cause_index},
        dims=["cluster_cause"],
    )
    scale_factor = xr.DataArray(
        yll_total_vals / rr_baseline_vals * constants.YLL_TO_MILLION_YLL,
        coords={"cluster_cause": cluster_cause_index},
        dims=["cluster_cause"],
    )

    # Vectorized YLL expression: (RR - RR^ref) * scale_factor
    yll_expr_all = (rr_interp_all - rr_ref) * scale_factor

    # Build store names array for vectorized store level constraint
    store_names = [
        health_stores.loc[(cluster, cause), "name"]
        for cluster, cause in cluster_cause_pairs
    ]

    # Verify all log_rr_totals exist and collect them
    total_exprs = []
    for cluster, cause in cluster_cause_pairs:
        if (cluster, cause) not in log_rr_totals:
            raise ValueError(
                f"No log_rr total from Stage 1 for cluster {cluster}, cause {cause}. "
                f"Check that food group stores exist and map to health clusters."
            )
        total_exprs.append(log_rr_totals[(cluster, cause)])

    # Add balance constraints - need to loop since total_exprs are separate expressions
    # But we can batch the store level constraints
    for (cluster, cause), label, total_expr in zip(
        cluster_cause_pairs, cluster_cause_labels, total_exprs
    ):
        log_interp = log_interp_all.sel(cluster_cause=label)
        m.add_constraints(
            log_interp == total_expr,
            name=f"health_delta_total_balance_c{cluster}_cause{cause}",
        )

    # Add store level constraints (still looped due to coordinate complexity)
    # But we pre-computed the vectorized yll_expr_all above
    for (cluster, cause), label, store_name in zip(
        cluster_cause_pairs, cluster_cause_labels, store_names
    ):
        yll_expr = yll_expr_all.sel(cluster_cause=label)
        store_var = store_level_var.sel(name=store_name)

        if value_per_yll == 0:
            m.add_constraints(
                store_var == yll_expr,
                name=f"health_store_level_c{cluster}_cause{cause}",
            )
        elif value_per_yll > 0:
            m.add_constraints(
                store_var >= yll_expr,
                name=f"health_store_level_c{cluster}_cause{cause}",
            )
        else:
            raise ValueError(f"value_per_yll must be non-negative, got {value_per_yll}")

    return len(cluster_cause_pairs)


# =============================================================================
# Main Entry Point
# =============================================================================


def add_health_objective(
    n: pypsa.Network,
    risk_breakpoints_path: str,
    cluster_cause_path: str,
    cause_log_path: str,
    cluster_summary_path: str,
    clusters_path: str,
    risk_factors: list[str],
    risk_cause_map: dict[str, list[str]],
    solver_name: str,
    value_per_yll: float,
) -> None:
    """Add health cost constraints to the optimization model.

    This implements the health cost formulation from docs/health.rst:

        Cost_{c,d}(x) = V * (YLL_{c,d} / RR_d(x^base)) * (RR_d(x) - RR_d^ref)

    where:
        - V = value_per_yll (USD per year of life lost)
        - YLL_{c,d} = baseline years of life lost for cluster c, disease d
        - RR_d(x) = relative risk at intake x (product over risk factors)
        - RR_d^ref = RR at TMREL (theoretical minimum risk exposure level)
        - x^base = baseline intake

    The implementation uses two-stage SOS2 interpolation to handle the
    nonlinear multiplicative combination of relative risks:

        Stage 1: Intake x_r → log(RR_{r,d})
        Stage 2: Σ_r log(RR_{r,d}) → RR_d → YLL store level

    Parameters
    ----------
    n
        The PyPSA network with health stores already added. Population data
        for health clusters is read from the network metadata.
    risk_breakpoints_path
        Path to CSV with (risk_factor, intake_g_per_day, cause, log_rr).
    cluster_cause_path
        Path to CSV with (health_cluster, cause, yll_total, log_rr_total_ref,
        log_rr_total_baseline).
    cause_log_path
        Path to CSV with (cause, log_rr_total, rr_total) breakpoints.
    cluster_summary_path
        Path to CSV with cluster metadata.
    clusters_path
        Path to CSV mapping countries to health clusters.
    risk_factors
        List of risk factors to include (e.g., ['fruits', 'vegetables', ...]).
    risk_cause_map
        Mapping from risk factor to list of affected causes.
    solver_name
        Solver name ('gurobi', 'highs', etc.).
    value_per_yll
        Monetary value per year of life lost (USD).
    """
    m = n.model

    # --- Load Data ---
    data = _load_health_data(
        n,
        risk_breakpoints_path,
        cluster_cause_path,
        cause_log_path,
        cluster_summary_path,
        clusters_path,
    )

    risk_breakpoints = data["risk_breakpoints"]
    cause_log_breakpoints = data["cause_log_breakpoints"]
    cluster_cause_metadata = data["cluster_cause_metadata"]
    cluster_lookup = data["cluster_lookup"]
    cluster_population = data["cluster_population"]

    logger.info(
        "Health data: %d risk breakpoints across %d risks / %d causes; %d cause breakpoints",
        len(risk_breakpoints),
        risk_breakpoints["risk_factor"].nunique(),
        risk_breakpoints["cause"].nunique(),
        len(cause_log_breakpoints),
    )

    # --- Validate Risk-Cause Pairs ---
    available_risks = set(risk_breakpoints["risk_factor"].unique())
    risk_cause_map = {
        r: causes for r, causes in risk_cause_map.items() if r in available_risks
    }

    allowed_pairs = {(r, c) for r, causes in risk_cause_map.items() for c in causes}
    rb_pairs = set(zip(risk_breakpoints["risk_factor"], risk_breakpoints["cause"]))
    missing_pairs = sorted(allowed_pairs - rb_pairs)
    if missing_pairs:
        text = ", ".join([f"{r}:{c}" for r, c in missing_pairs])
        raise ValueError(f"Risk breakpoints missing required pairs: {text}")

    # --- Build Store Map ---
    # Map food group stores to health clusters with per-capita coefficients.
    store_level_var = m.variables["Store-e"].sel(snapshot="now")

    store_map = _build_store_to_cluster_map(
        n.stores.static,
        risk_factors,
        cluster_lookup,
        cluster_population,
    )

    if store_map.empty:
        logger.info("No food group stores map to health risk factors; skipping")
        return

    logger.info(
        "Health intake mapping: %d stores -> %d cluster-risk pairs across %d clusters",
        len(store_map),
        len(store_map[["cluster", "risk_factor"]].drop_duplicates()),
        store_map["cluster"].nunique(),
    )

    # --- Stage 1: Store Level → log(RR) ---
    intake_data = _build_intake_breakpoints(risk_breakpoints)
    intake_groups = _group_cluster_risk_pairs(store_map, intake_data)

    log_rr_totals = _add_stage1_constraints(
        m,
        store_map,
        intake_groups,
        intake_data,
        store_level_var,
        solver_name,
        value_per_yll,
    )

    # --- Stage 2: log(RR) → YLL Store ---
    cause_breakpoints = _build_cause_breakpoints(cause_log_breakpoints)
    log_total_groups, cluster_cause_data = _group_cluster_cause_pairs(
        cluster_cause_metadata, cause_breakpoints, cluster_population
    )

    logger.info(
        "Health risk aggregation: %d (cluster, cause) pairs grouped into %d log-RR grids",
        len(cluster_cause_data),
        len(log_total_groups),
    )

    # Get health store mapping
    health_stores = (
        n.stores.static[
            n.stores.static["carrier"].notna()
            & n.stores.static["carrier"].str.startswith("yll_")
        ]
        .reset_index()
        .set_index(["health_cluster", "cause"])
    )

    constraints_added = _add_stage2_constraints(
        m,
        log_rr_totals,
        log_total_groups,
        cluster_cause_data,
        health_stores,
        store_level_var,
        value_per_yll,
    )

    logger.info("Added %d health store level constraints", constraints_added)
